{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_2D_oat.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('train_2D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Images'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Test'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/models'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/valid.txt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(config['data']['dir'])\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path('/home/suze/seed_images/Data_for_ML_Test/test_model/Oat_test')\n",
    "#path = Path('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train')\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = path/config['data']['images']\n",
    "path_lbl = path/config['data']['labels']\n",
    "valid = config['data']['valid']\n",
    "classes = int(config['data']['classes'])\n",
    "model_name = config['data']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(path_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_img = path/'Images' #should later be provided through ini file\n",
    "#path_lbl = path/'NewLabels' #should later be provided through ini file\n",
    "#path_img\n",
    "#path_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names=get_image_files(path_img)\n",
    "img_names[:3]\n",
    "lbl_names=get_image_files(path_lbl)\n",
    "lbl_names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function to filter out images that only contain background; used to calculate appropriate weights; \n",
    "function returns the images that also contain seed in a list\"\"\"\n",
    "def filter_background(img_list):\n",
    "    include=[]\n",
    "    for img in img_list:\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total=(torch.unique(mask.data, return_counts=True))\n",
    "        if not count_total[0].tolist() == [0]:\n",
    "            include.append(img)\n",
    "    return include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function to filter out images that only contain background; used to filter in the data block API; \n",
    "function has to return a boolean\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = open_mask(get_mask(img_names[300]))\n",
    "#mask.data\n",
    "#count_total=(torch.unique(mask.data, return_counts=True))\n",
    "#count_total[0].tolist()\n",
    "#len(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include = filter_background(img_names)\n",
    "#len(include)\n",
    "#mask.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_list2\n",
    "img = open_image(img_names[4])\n",
    "img.show()\n",
    "#src_size = np.array(mask.shape[1:])\n",
    "#src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mask(img_list):\n",
    "    img_list = filter_background(img_list)\n",
    "    count_classes = defaultdict(int)\n",
    "    for img in img_list:\n",
    "        #return the occurence of every class in the mask for an image\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total = torch.unique(mask.data, return_counts=True)\n",
    "        classes=count_total[0].tolist()\n",
    "        count_real=count_total[1].tolist()\n",
    "        for x, y in zip(classes, count_real):\n",
    "            count_classes[x] += y\n",
    "    return count_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_train(path_img, valid):\n",
    "    #create a list of images not in validation set\n",
    "    path = path_img.parent\n",
    "    valid_names = loadtxt_str(path/valid)\n",
    "    img_names = get_image_files(path_img)\n",
    "    train_img = list(filter(lambda x: (x.name not in valid_names), img_names))\n",
    "    return train_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = img_train(path_img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time classes_count = count_mask(train_img)\n",
    "classes_count\n",
    "#slow for big sets, make faster?\n",
    "#defaultdict(int, {0: 2403155103, 1: 1246213160, 2: 149865917, 3: 66084620})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all classes are represented in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes_count.items()) != classes: print('Not all classes present in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append class counts in a list \n",
    "counts = []\n",
    "for c in classes_count:\n",
    "    counts.append(classes_count[c])\n",
    "counts\n",
    "#counts = [2403155103, 1246213160, 149865917, 66084620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ratios =[min(counts)/x for x in counts]\n",
    "weight_ratios\n",
    "#weight_ratios = [0.027499107285044847, 0.05302834388299992, 0.44095830007832937, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for training\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "wd = float(config['training_settings']['wd'])\n",
    "lr = float(config['training_settings']['lr'])\n",
    "size_s = int(config['training_settings']['size_s'])\n",
    "size_m = int(config['training_settings']['size_m'])\n",
    "size_l = int(config['training_settings']['size_l'])\n",
    "epochs_s1 = int(config['training_settings']['epochs_s1'])\n",
    "epochs_s2 = int(config['training_settings']['epochs_s2'])\n",
    "epochs_m = int(config['training_settings']['epochs_m'])\n",
    "epochs_l = int(config['training_settings']['epochs_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "src = (SegmentationItemList.from_folder(path)\n",
    "       .filter_by_folder(include=config['data']['images'])\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file(valid)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(get_transforms(), \n",
    "        tfm_y=True, \n",
    "        size=size_s)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based of fastai foreground_acc\n",
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0 #not interested in background\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set class weights\n",
    "class_weights=torch.FloatTensor(weight_ratios)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the block below to find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_find(learn)\n",
    "#learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.utils.mem import *\n",
    "#free = gpu_mem_get_free_no_cache()\n",
    "# the max size of bs depends on the available GPU RAM\n",
    "#if free > 8200: bs=8\n",
    "#else:           bs=8\n",
    "#print(f\"using bs={bs}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.device_count()\n",
    "#torch.cuda.is_available()\n",
    "#torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_1')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s2, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = (src.transform(get_transforms(), tfm_y=True, size=size_m)\n",
    "       .databunch(bs=bs)\n",
    "       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data_med, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_m, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-med')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_L = (src.transform(get_transforms(), tfm_y=True, size=size_l)\n",
    "       .databunch(bs=bs)\n",
    "       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-med');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_l, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tfms = (get_transforms(xtra_tfms=crop(size=280)))\n",
    "#tfms = get_transforms()\n",
    "#tfms = (get_transforms(xtra_tfms=crop_pad(size=64, padding_mode='border')))\n",
    "#tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop = (src.transform(tfms, tfm_y=True, size=256)\n",
    "#       .databunch(bs=8)\n",
    "#       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms(data_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Interpretations on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.interpret import *\n",
    "interp = SegmentationInterpretation.from_learner(learn)\n",
    "#interp = Interpretation.from_learner(learn)\n",
    "#interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_s,size_s))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(single_img_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interp_show_new(self, ims:ImageSegment, classes:Collection=None, sz:int=20, cmap='tab20',\n",
    "                    title_suffix:str=None):\n",
    "        \"Show ImageSegment with color mapping labels\"\n",
    "        fig,axes=plt.subplots(1,2,figsize=(sz,sz))\n",
    "        np_im = to_np(ims.data).copy()\n",
    "        # tab20 - qualitative colormaps support max of 20 distinc colors\n",
    "        # if len(classes) > 20 close idxs map to same color\n",
    "        # image\n",
    "        if classes is not None:\n",
    "            class_idxs = [self.c2i[c] for c in classes]\n",
    "            mask = np.max(np.stack([np_im==i for i in class_idxs]),axis=0)\n",
    "            np_im = (np_im*mask).astype(np.float)\n",
    "            np_im[np.where(mask==0)] = np.nan\n",
    "        im=axes[0].imshow(np_im[0], cmap=cmap)\n",
    "\n",
    "        # labels\n",
    "        np_im_labels = list(np.unique(np_im[~np.isnan(np_im)]))\n",
    "        c = len(np_im_labels); n = math.ceil(np.sqrt(c))\n",
    "        label_im = np.array(np_im_labels + [np.nan]*(n**2-c)).reshape(n,n)\n",
    "        axes[1].imshow(label_im, cmap=cmap)\n",
    "        for i,l in enumerate([self.i2c[l] for l in np_im_labels]):\n",
    "            div,mod=divmod(i,n)\n",
    "            #l = \"\\n\".join(wrap(l,10)) if len(l) > 10 else l #bug fix\n",
    "            axes[1].text(mod, div, f\"{l}\", ha='center', color='white', fontdict={'size':sz})\n",
    "\n",
    "        if title_suffix:\n",
    "            axes[0].set_title(f\"{title_suffix}_imsegment\")\n",
    "            axes[1].set_title(f\"{title_suffix}_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def show_xyz_new(self, i, classes:list=None, sz=10):\n",
    "        'show (image, true and pred) from self.ds with color mappings, optionally only plot'\n",
    "        funcType = types.MethodType\n",
    "        self._interp_show = funcType(_interp_show_new, self)\n",
    "        x,y = self.ds[i]\n",
    "        self.ds.show_xys([x],[y], figsize=(sz/2,sz/2))\n",
    "        self._interp_show(ImageSegment(self.y_true[i]), classes, sz=sz, title_suffix='true')\n",
    "        self._interp_show(ImageSegment(self.pred_class[i][None,:]), classes, sz=sz, title_suffix='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from fastai.vision.interpret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_2D_oat.ini']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provide config file name through cmd line\n",
    "config = configparser.ConfigParser()\n",
    "config.read('pred_2D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(config['data']['dir_to_model'])\n",
    "model = (config['data']['model'])\n",
    "#raw_img = config['data']['raw_images']\n",
    "#pred_lbl = config['data']['pred_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, file=model)\n",
    "#learn.data.single_ds.tfmargs['size'] = None #ensure match to new image size, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiList = (SegmentationItemList.from_folder(path)\n",
    "       .split_by_folder(train='Images', valid='Test')\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (lsiList.transform(get_transforms(), \n",
    "        tfm_y=True, \n",
    "        size=300)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#siList = SegmentationItemList.from_folder(path=path)#.filter_by_folder(include=raw_img)\n",
    "#siLists = siList.split_by_folder(train='Images', valid='Test')\n",
    "#lsiList = siLists.label_from_func(get_mask, classes=list(range(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = (lsiList.transform(get_transforms(), size=(598,744))\n",
    "#             .databunch(bs=8))\n",
    "#data_test = (lsiList.databunch(bs=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test.normalize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_dl = data_test.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.015699659, tensor(0.9618)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(data_test.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret\n",
    "interp = SegmentationInterpretation.from_learner(learn)#,ds_type=DatasetType.Valid)\n",
    "#interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_losses, top_idxs = interp.top_losses(sizes=(size_s,size_s))\n",
    "#top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0.873701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>0.868590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>0.522630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
