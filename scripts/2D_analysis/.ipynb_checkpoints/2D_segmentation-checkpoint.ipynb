{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('train_2D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(config['data']['dir'])\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = path/config['data']['images']\n",
    "path_lbl = path/config['data']['labels']\n",
    "valid = config['data']['valid']\n",
    "classes = int(config['data']['classes'])\n",
    "model_name = config['data']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names=get_image_files(path_img)\n",
    "img_names[:3]\n",
    "lbl_names=get_image_files(path_lbl)\n",
    "lbl_names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to calculate appropriate weights; \n",
    "function returns the images that also contain seed in a list\"\"\"\n",
    "def filter_background(img_list):\n",
    "    include=[]\n",
    "    for img in img_list:\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total=(torch.unique(mask.data, return_counts=True))\n",
    "        if not count_total[0].tolist() == [0]:\n",
    "            include.append(img)\n",
    "    return include\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to filter in the data block API; \n",
    "function has to return a boolean\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def count_mask(img_list):\n",
    "    img_list = filter_background(img_list)\n",
    "    count_classes = defaultdict(int)\n",
    "    for img in img_list:\n",
    "        #return the occurence of every class in the mask for an image\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total = torch.unique(mask.data, return_counts=True)\n",
    "        classes=count_total[0].tolist()\n",
    "        count_real=count_total[1].tolist()\n",
    "        for x, y in zip(classes, count_real):\n",
    "            count_classes[x] += y\n",
    "    return count_classes\n",
    "\n",
    "def img_train(path_img, valid):\n",
    "    #create a list of images not in validation set\n",
    "    path = path_img.parent\n",
    "    valid_names = loadtxt_str(path/valid)\n",
    "    img_names = get_image_files(path_img)\n",
    "    train_img = list(filter(lambda x: (x.name not in valid_names), img_names))\n",
    "    return train_img\n",
    "\n",
    "#based of fastai foreground_acc\n",
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0 #not interested in background\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = img_train(path_img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time classes_count = count_mask(train_img)\n",
    "#classes_count\n",
    "classes_count = defaultdict(int, {0: 6594921884, 1: 3243614049, 2: 355819488, 3: 205649589}) #rotation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all classes are represented in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes_count.items()) != classes: print('Not all classes present in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append class counts in a list \n",
    "counts = []\n",
    "for c in classes_count:\n",
    "    counts.append(classes_count[c])\n",
    "counts\n",
    "#counts = [6594921884, 3243614049, 355819488, 205649589] #rotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ratios =[min(counts)/x for x in counts]\n",
    "weight_ratios\n",
    "#weight_ratios = [0.031183021211961332, 0.06340137448331788, 0.5779604432458741, 1.0] #rotation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for training\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "wd = float(config['training_settings']['wd'])\n",
    "lr = float(config['training_settings']['lr'])\n",
    "size_s = int(config['training_settings']['size_s'])\n",
    "size_m = int(config['training_settings']['size_m'])\n",
    "size_l = int(config['training_settings']['size_l'])\n",
    "epochs_s1 = int(config['training_settings']['epochs_s1'])\n",
    "epochs_s2 = int(config['training_settings']['epochs_s2'])\n",
    "epochs_m = int(config['training_settings']['epochs_m'])\n",
    "epochs_l = int(config['training_settings']['epochs_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "src = (SegmentationItemList.from_folder(path)\n",
    "       .filter_by_folder(include=config['data']['images'])\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file(valid)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#        tfm_y=True, \n",
    "#        size=size_s)\n",
    "#        .databunch(bs=bs)\n",
    "#        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_s)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set class weights\n",
    "class_weights=torch.FloatTensor(weight_ratios)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the block below to find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_1')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s2, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_med = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#                            tfm_y=True, size=size_m)\n",
    "#       .databunch(bs=bs)\n",
    "#       .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_m)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_med, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_m, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-med')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_L = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#                                tfm_y=True, size=size_l)\n",
    "#       .databunch(bs=bs)\n",
    "#       .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_L = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_l)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_L, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-med');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_l, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tfms = (get_transforms(xtra_tfms=crop(size=280)))\n",
    "#tfms = get_transforms()\n",
    "#tfms = (get_transforms(xtra_tfms=crop_pad(size=64, padding_mode='border')))\n",
    "#tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop = (src.transform(tfms, tfm_y=True, size=256)\n",
    "#       .databunch(bs=8)\n",
    "#       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms(data_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Interpretations on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.interpret import *\n",
    "interp = SegmentationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(data_L.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bug fix fastai library function\n",
    "def _interp_show_new(self, ims:ImageSegment, classes:Collection=None, sz:int=20, cmap='tab20',\n",
    "                    title_suffix:str=None):\n",
    "        \"Show ImageSegment with color mapping labels\"\n",
    "        fig,axes=plt.subplots(1,2,figsize=(sz,sz))\n",
    "        np_im = to_np(ims.data).copy()\n",
    "        # tab20 - qualitative colormaps support max of 20 distinc colors\n",
    "        # if len(classes) > 20 close idxs map to same color\n",
    "        # image\n",
    "        if classes is not None:\n",
    "            class_idxs = [self.c2i[c] for c in classes]\n",
    "            mask = np.max(np.stack([np_im==i for i in class_idxs]),axis=0)\n",
    "            np_im = (np_im*mask).astype(np.float)\n",
    "            np_im[np.where(mask==0)] = np.nan\n",
    "        im=axes[0].imshow(np_im[0], cmap=cmap)\n",
    "\n",
    "        # labels\n",
    "        np_im_labels = list(np.unique(np_im[~np.isnan(np_im)]))\n",
    "        c = len(np_im_labels); n = math.ceil(np.sqrt(c))\n",
    "        label_im = np.array(np_im_labels + [np.nan]*(n**2-c)).reshape(n,n)\n",
    "        axes[1].imshow(label_im, cmap=cmap)\n",
    "        for i,l in enumerate([self.i2c[l] for l in np_im_labels]):\n",
    "            div,mod=divmod(i,n)\n",
    "            #l = \"\\n\".join(wrap(l,10)) if len(l) > 10 else l #bug fix\n",
    "            axes[1].text(mod, div, f\"{l}\", ha='center', color='white', fontdict={'size':sz})\n",
    "\n",
    "        if title_suffix:\n",
    "            axes[0].set_title(f\"{title_suffix}_imsegment\")\n",
    "            axes[1].set_title(f\"{title_suffix}_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def show_xyz_new(self, i, classes:list=None, sz=10):\n",
    "        'show (image, true and pred) from self.ds with color mappings, optionally only plot'\n",
    "        funcType = types.MethodType\n",
    "        self._interp_show = funcType(_interp_show_new, self)\n",
    "        x,y = self.ds[i]\n",
    "        self.ds.show_xys([x],[y], figsize=(sz/2,sz/2))\n",
    "        self._interp_show(ImageSegment(self.y_true[i]), classes, sz=sz, title_suffix='true')\n",
    "        self._interp_show(ImageSegment(self.pred_class[i][None,:]), classes, sz=sz, title_suffix='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution of Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If testing the model on a new session, make sure to rerun the specified functions in the notebook above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from fastai.vision.interpret import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('2D_test.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(config['data']['dir_to_model'])\n",
    "model = (config['data']['model'])\n",
    "train_set = (config['data']['train'])\n",
    "test_set = (config['data']['test'])\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "size_l = int(config['training_settings']['size_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, file=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiList = (SegmentationItemList.from_folder(path)\n",
    "       .split_by_folder(train=train_set, valid=test_set)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (lsiList.transform( \n",
    "        tfm_y=True, \n",
    "        size=size_l)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_dl = data_test.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(data_test.valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = SegmentationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
