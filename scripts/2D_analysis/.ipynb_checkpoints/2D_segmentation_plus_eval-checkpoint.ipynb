{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### %reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_2D_oat.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('train_2D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Images'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Test'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/models'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_norot_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_no_tfms.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/valid.txt')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(config['data']['dir'])\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path('/home/suze/seed_images/Data_for_ML_Test/test_model/Oat_test')\n",
    "#path = Path('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train')\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = path/config['data']['images']\n",
    "path_lbl = path/config['data']['labels']\n",
    "valid = config['data']['valid']\n",
    "classes = int(config['data']['classes'])\n",
    "model_name = config['data']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_img = path/'Images' #should later be provided through ini file\n",
    "#path_lbl = path/'NewLabels' #should later be provided through ini file\n",
    "#path_img\n",
    "#path_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels/OB5_zx_0298.tif'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels/OB6_x_0106.tif'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels/OM1_7_0503.tif')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_names=get_image_files(path_img)\n",
    "img_names[:3]\n",
    "lbl_names=get_image_files(path_lbl)\n",
    "lbl_names[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to calculate appropriate weights; \n",
    "function returns the images that also contain seed in a list\"\"\"\n",
    "def filter_background(img_list):\n",
    "    include=[]\n",
    "    for img in img_list:\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total=(torch.unique(mask.data, return_counts=True))\n",
    "        if not count_total[0].tolist() == [0]:\n",
    "            include.append(img)\n",
    "    return include\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to filter in the data block API; \n",
    "function has to return a boolean\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def count_mask(img_list):\n",
    "    img_list = filter_background(img_list)\n",
    "    count_classes = defaultdict(int)\n",
    "    for img in img_list:\n",
    "        #return the occurence of every class in the mask for an image\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total = torch.unique(mask.data, return_counts=True)\n",
    "        classes=count_total[0].tolist()\n",
    "        count_real=count_total[1].tolist()\n",
    "        for x, y in zip(classes, count_real):\n",
    "            count_classes[x] += y\n",
    "    return count_classes\n",
    "\n",
    "def img_train(path_img, valid):\n",
    "    #create a list of images not in validation set\n",
    "    path = path_img.parent\n",
    "    valid_names = loadtxt_str(path/valid)\n",
    "    img_names = get_image_files(path_img)\n",
    "    train_img = list(filter(lambda x: (x.name not in valid_names), img_names))\n",
    "    return train_img\n",
    "\n",
    "#based of fastai foreground_acc\n",
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0 #not interested in background\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = open_mask(get_mask(img_names[300]))\n",
    "#mask.data\n",
    "#count_total=(torch.unique(mask.data, return_counts=True))\n",
    "#count_total[0].tolist()\n",
    "#len(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include = filter_background(img_names)\n",
    "#len(include)\n",
    "#mask.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_list2\n",
    "img = open_image(img_names[4])\n",
    "img.show()\n",
    "#src_size = np.array(mask.shape[1:])\n",
    "#src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = img_train(path_img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time classes_count = count_mask(train_img)\n",
    "classes_count\n",
    "#slow for big sets, make faster?\n",
    "#defaultdict(int, {0: 6594921884, 1: 3243614049, 2: 355819488, 3: 205649589}) #rotation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all classes are represented in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes_count.items()) != classes: print('Not all classes present in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append class counts in a list \n",
    "counts = []\n",
    "for c in classes_count:\n",
    "    counts.append(classes_count[c])\n",
    "counts\n",
    "#counts = [6594921884, 3243614049, 355819488, 205649589] #rotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ratios =[min(counts)/x for x in counts]\n",
    "weight_ratios\n",
    "#weight_ratios = [0.031183021211961332, 0.06340137448331788, 0.5779604432458741, 1.0] #rotation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for training\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "wd = float(config['training_settings']['wd'])\n",
    "lr = float(config['training_settings']['lr'])\n",
    "size_s = int(config['training_settings']['size_s'])\n",
    "size_m = int(config['training_settings']['size_m'])\n",
    "size_l = int(config['training_settings']['size_l'])\n",
    "epochs_s1 = int(config['training_settings']['epochs_s1'])\n",
    "epochs_s2 = int(config['training_settings']['epochs_s2'])\n",
    "epochs_m = int(config['training_settings']['epochs_m'])\n",
    "epochs_l = int(config['training_settings']['epochs_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "src = (SegmentationItemList.from_folder(path)\n",
    "       .filter_by_folder(include=config['data']['images'])\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file(valid)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#        tfm_y=True, \n",
    "#        size=size_s)\n",
    "#        .databunch(bs=bs)\n",
    "#        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_s)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set class weights\n",
    "class_weights=torch.FloatTensor(weight_ratios)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the block below to find a good learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.utils.mem import *\n",
    "#free = gpu_mem_get_free_no_cache()\n",
    "# the max size of bs depends on the available GPU RAM\n",
    "#if free > 8200: bs=8\n",
    "#else:           bs=8\n",
    "#print(f\"using bs={bs}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.device_count()\n",
    "#torch.cuda.is_available()\n",
    "#torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s1, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_1')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s2, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_med = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#                            tfm_y=True, size=size_m)\n",
    "#       .databunch(bs=bs)\n",
    "#       .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_m)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_med, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_m, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-med')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_L = (src.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)),\n",
    "#                                tfm_y=True, size=size_l)\n",
    "#       .databunch(bs=bs)\n",
    "#       .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_L = (src.transform(\n",
    "        tfm_y=True, \n",
    "        size=size_l)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_L, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn)\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-med');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_l, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tfms = (get_transforms(xtra_tfms=crop(size=280)))\n",
    "#tfms = get_transforms()\n",
    "#tfms = (get_transforms(xtra_tfms=crop_pad(size=64, padding_mode='border')))\n",
    "#tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop = (src.transform(tfms, tfm_y=True, size=256)\n",
    "#       .databunch(bs=8)\n",
    "#       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_transforms(data_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_crop.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Interpretations on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.interpret import *\n",
    "interp = SegmentationInterpretation.from_learner(learn)\n",
    "#interp = Interpretation.from_learner(learn)\n",
    "#interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(data_L.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(single_img_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interp_show_new(self, ims:ImageSegment, classes:Collection=None, sz:int=20, cmap='tab20',\n",
    "                    title_suffix:str=None):\n",
    "        \"Show ImageSegment with color mapping labels\"\n",
    "        fig,axes=plt.subplots(1,2,figsize=(sz,sz))\n",
    "        np_im = to_np(ims.data).copy()\n",
    "        # tab20 - qualitative colormaps support max of 20 distinc colors\n",
    "        # if len(classes) > 20 close idxs map to same color\n",
    "        # image\n",
    "        if classes is not None:\n",
    "            class_idxs = [self.c2i[c] for c in classes]\n",
    "            mask = np.max(np.stack([np_im==i for i in class_idxs]),axis=0)\n",
    "            np_im = (np_im*mask).astype(np.float)\n",
    "            np_im[np.where(mask==0)] = np.nan\n",
    "        im=axes[0].imshow(np_im[0], cmap=cmap)\n",
    "\n",
    "        # labels\n",
    "        np_im_labels = list(np.unique(np_im[~np.isnan(np_im)]))\n",
    "        c = len(np_im_labels); n = math.ceil(np.sqrt(c))\n",
    "        label_im = np.array(np_im_labels + [np.nan]*(n**2-c)).reshape(n,n)\n",
    "        axes[1].imshow(label_im, cmap=cmap)\n",
    "        for i,l in enumerate([self.i2c[l] for l in np_im_labels]):\n",
    "            div,mod=divmod(i,n)\n",
    "            #l = \"\\n\".join(wrap(l,10)) if len(l) > 10 else l #bug fix\n",
    "            axes[1].text(mod, div, f\"{l}\", ha='center', color='white', fontdict={'size':sz})\n",
    "\n",
    "        if title_suffix:\n",
    "            axes[0].set_title(f\"{title_suffix}_imsegment\")\n",
    "            axes[1].set_title(f\"{title_suffix}_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "def show_xyz_new(self, i, classes:list=None, sz=10):\n",
    "        'show (image, true and pred) from self.ds with color mappings, optionally only plot'\n",
    "        funcType = types.MethodType\n",
    "        self._interp_show = funcType(_interp_show_new, self)\n",
    "        x,y = self.ds[i]\n",
    "        self.ds.show_xys([x],[y], figsize=(sz/2,sz/2))\n",
    "        self._interp_show(ImageSegment(self.y_true[i]), classes, sz=sz, title_suffix='true')\n",
    "        self._interp_show(ImageSegment(self.pred_class[i][None,:]), classes, sz=sz, title_suffix='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from fastai.vision.interpret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_2D_oat.ini']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provide config file name through cmd line\n",
    "config = configparser.ConfigParser()\n",
    "config.read('pred_2D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(config['data']['dir_to_model'])\n",
    "model = (config['data']['model'])\n",
    "#raw_img = config['data']['raw_images']\n",
    "#pred_lbl = config['data']['pred_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, file=model)\n",
    "#learn.data.single_ds.tfmargs['size'] = None #ensure match to new image size, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiList = (SegmentationItemList.from_folder(path)\n",
    "       .split_by_folder(train='Images', valid='Test')\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_test = (lsiList.transform(get_transforms(flip_vert=True, \n",
    "#                                     max_rotate=90, \n",
    "#                                     max_zoom=1.3, \n",
    "#                                     xtra_tfms=zoom_crop(scale=(1.,3.), do_rand=True, p=0.1)), \n",
    "#        tfm_y=True, \n",
    "#        size=300)\n",
    "#        .databunch(bs=bs)\n",
    "#        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (lsiList.transform( \n",
    "        tfm_y=True, \n",
    "        size=300)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_dl = data_test.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (6347 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300)\n",
       "Path: /home/suze/seed_images/Data_for_ML_Test/train/Oat_test;\n",
       "\n",
       "Valid: LabelList (2503 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300)\n",
       "Path: /home/suze/seed_images/Data_for_ML_Test/train/Oat_test;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In [1]: import torch In [2]: torch.cuda.current_device() Out[2]: 0 In [3]: torch.cuda.device(0) \n",
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.032803953, tensor(0.9462)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(data_test.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret\n",
    "interp = SegmentationInterpretation.from_learner(learn)#,ds_type=DatasetType.Valid)\n",
    "#interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "#top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>0.851738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>0.284646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
