{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import SequentialSampler\n",
    "from fastai3D import mysampler\n",
    "import configparser\n",
    "#from fastai3D import functions\n",
    "#from fastai3D import loader #does not comply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_3D_oat.ini']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('train_3D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/NewLabels'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Images'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Test'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/models'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_3D_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_norot_big.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/Oat_no_tfms.pkl'),\n",
       " PosixPath('/home/suze/seed_images/Data_for_ML_Test/train/Oat_test/valid.txt')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(config['data']['dir'])\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path = Path('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train') #suze_device\n",
    "#path = Path('/home/suze/seed_images/Data_for_ML_Test//test_model/Barley_test')\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_img = path/'Images' #should later be provided through ini file\n",
    "#path_lbl = path/'Labels' #should later be provided through ini file #suze_device\n",
    "#path_lbl = path/'NewLabels' #should later be provided through ini file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = path/config['data']['images']\n",
    "path_lbl = path/config['data']['labels']\n",
    "valid = config['data']['valid']\n",
    "classes = int(config['data']['classes'])\n",
    "model_name = config['data']['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment in case of file issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_names=get_image_files(path_img)\n",
    "#img_names[:3]\n",
    "#lbl_names=get_image_files(path_lbl)\n",
    "#lbl_names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to find mask matching the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_mask(x):\n",
    "#    return PosixPath(str(x)\n",
    "#            .replace('train','NewLabels')\n",
    "#            .replace('valid','NewLabels'))\n",
    "            #.replace('resampled', 'labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment for error testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = open_mask(get_mask(img_names[0]))\n",
    "#mask.show()\n",
    "#img = open_image(img_names[0])\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_size = np.array(mask.shape[1:])\n",
    "#src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name\n",
    "\n",
    "def filter_background(img_list):\n",
    "    include=[]\n",
    "    for img in img_list:\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total=(torch.unique(mask.data, return_counts=True))\n",
    "        if not count_total[0].tolist() == [0]:\n",
    "            include.append(img)\n",
    "    return include\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to filter in the API; \n",
    "function has to return a boolean\n",
    "slow, every mask has to be opened to check the values\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def count_mask(img_list):\n",
    "    img_list = filter_background(img_list)\n",
    "    count_classes = defaultdict(int)\n",
    "    for img in img_list:\n",
    "        #return the occurence of every class in the mask for an image\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total = torch.unique(mask.data, return_counts=True)\n",
    "        classes=count_total[0].tolist()\n",
    "        count_real=count_total[1].tolist()\n",
    "        for x, y in zip(classes, count_real):\n",
    "            count_classes[x] += y\n",
    "    return count_classes\n",
    "\n",
    "def img_train(path_img, valid):\n",
    "    #create a list of images not in validation set\n",
    "    path = path_img.parent\n",
    "    valid_names = loadtxt_str(path/valid)\n",
    "    img_names = get_image_files(path_img)\n",
    "    train_img = list(filter(lambda x: (x.name not in valid_names), img_names))\n",
    "    return train_img\n",
    "\n",
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0 #not interested in background\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate appropriate weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = img_train(path_img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time classes_count = count_mask(train_img)\n",
    "#classes_count\n",
    "#slow for big sets, make faster?\n",
    "classes_count = defaultdict(int, {0: 2257029582, 1: 1081204683, 2: 118606496, 3: 68549863})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all classes are represented in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes_count.items()) != classes: print('Not all classes present in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append occurences in a list \n",
    "#seems redundant\n",
    "#counts = []\n",
    "#for c in classes_count:\n",
    "#    counts.append(classes_count[c])\n",
    "#counts\n",
    "counts = [2257029582, 1081204683, 118606496, 68549863]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ratios =[min(counts)/x for x in counts]\n",
    "weight_ratios\n",
    "#weight_ratios = [0.030371716678722734, 0.06340137448331788, 0.5779604432458741, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load in data and apply transformations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for training\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "wd = float(config['training_settings']['wd'])\n",
    "lr = float(config['training_settings']['lr'])\n",
    "size_s = int(config['training_settings']['size_s'])\n",
    "size_m = int(config['training_settings']['size_m'])\n",
    "size_l = int(config['training_settings']['size_l'])\n",
    "epochs_s1 = int(config['training_settings']['epochs_s1'])\n",
    "epochs_s2 = int(config['training_settings']['epochs_s2'])\n",
    "epochs_m = int(config['training_settings']['epochs_m'])\n",
    "epochs_l = int(config['training_settings']['epochs_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path)\n",
    "       .filter_by_folder(include=config['data']['images'])\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file(valid)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = (src.transform(get_transforms(), tfm_y=True, size=128)\n",
    "#       .databunch(bs=1)\n",
    "#       .normalize())\n",
    "#TO DO: transforms need to be applied after batching \n",
    "#data = (src.transform(tfm_y=True,\n",
    "#        size=size_s)\n",
    "#       .databunch(bs=1)\n",
    "#       .normalize())\n",
    "#data.show_batch(4, figsize=(10,7))\n",
    "#data.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(tfm_y=True,\n",
    "        size=size_s)\n",
    "       .databunch(bs=1)\n",
    "       .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to custom sampler for order within batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai3D import loader\n",
    "data.train_dl = data.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.add_tfm(rotate(degrees=(90,90), p=1))\n",
    "#data.train_ds.transform(rotate(degrees=(90,90), p=.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(4, figsize=(10,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#print(data.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set class weights\n",
    "class_weights=torch.FloatTensor(weight_ratios)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()\n",
    "#learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 3D layers to learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False,\n",
    "               is_2d:bool = False, trans_2d:bool = False,\n",
    "               norm_type:Optional[NormType]=NormType.Batch,  use_activ:bool=True, leaky:float=None,\n",
    "               transpose:bool=False, init:Callable=nn.init.kaiming_normal_, self_attention:bool=False):\n",
    "    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and batchnorm (if `bn`) layers.\"\n",
    "    if padding is None: padding = (ks-1)//2 if not transpose else 0\n",
    "    bn = norm_type in (NormType.Batch, NormType.BatchZero)\n",
    "    if bias is None: bias = not bn\n",
    "    conv_func = nn.ConvTranspose3d if transpose else nn.Conv1d if is_1d else nn.ConvTranspose2d if trans_2d else nn.Conv2d if is_2d else nn.Conv3d\n",
    "    conv = init_default(conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding), init)\n",
    "    if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n",
    "    elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n",
    "    layers = [conv]\n",
    "    if use_activ: layers.append(relu(True, leaky=leaky))\n",
    "    if bn: layers.append((nn.BatchNorm1d if is_1d else nn.BatchNorm2d if is_2d else nn.BatchNorm3d)(nf))\n",
    "    if self_attention: layers.append(SelfAttention(nf))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def stack_1(x):\n",
    "    return torch.stack(tuple(x), dim = 3)\n",
    "\n",
    "def stack_2(x):\n",
    "    return torch.unsqueeze(x,0)\n",
    "\n",
    "#stack_layers = nn.Sequential(\n",
    "#    Lambda(lambda x: torch.stack(tuple(x), dim = 3)),\n",
    "#    Lambda(lambda x: torch.unsqueeze(x,0))\n",
    "#)\n",
    "\n",
    "stack_layers = nn.Sequential(\n",
    "    Lambda(stack_1),\n",
    "    Lambda(stack_2)\n",
    ")\n",
    "\n",
    "\n",
    "def unstack_1(x):\n",
    "    return torch.squeeze(x, dim = 0)\n",
    "\n",
    "def unstack_2(x):\n",
    "    return torch.stack(torch.unbind(x, dim = 3), dim =0)\n",
    "\n",
    "#unstack_layers = nn.Sequential(\n",
    "#    Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "#    Lambda(lambda x: torch.stack(torch.unbind(x, dim = 3), dim =0))\n",
    "#)\n",
    "\n",
    "unstack_layers = nn.Sequential(\n",
    "    Lambda(unstack_1),\n",
    "    Lambda(unstack_2)\n",
    ")\n",
    "\n",
    "just3D = nn.Sequential(conv3_layer(4,64),conv3_layer(64,4))\n",
    "\n",
    "mini3DUnet = SequentialEx(conv3_layer(4, 64),\n",
    "                          conv3_layer(64,128),\n",
    "                          conv3_layer(128,64),\n",
    "                         conv3_layer(64,4),\n",
    "                         MergeLayer())\n",
    "\n",
    "midi3DUnet = SequentialEx(conv3_layer(4,64),\n",
    "                          SequentialEx(conv3_layer(64,128),\n",
    "                                       (SequentialEx(conv3_layer(128,128), MergeLayer())),\n",
    "                                      conv3_layer(128,64), MergeLayer()),\n",
    "                         conv3_layer(64,4),\n",
    "                         MergeLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.append(stack_layers.cuda())\n",
    "learn.model.append(midi3DUnet.cuda())\n",
    "learn.model.append(unstack_layers.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(cyc_len=1, max_lr=lr)\n",
    "learn.fit_one_cycle(cyc_len=epochs_s1, max_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_1')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = slice(lr/100,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s2, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = (src.transform(tfm_y=True, size=size_m)\n",
    "       .databunch(bs=1)\n",
    "       .normalize(imagenet_stats))\n",
    "data_med.train_dl = data_med.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data_med.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_med, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learn.model.append(stack_layers.cuda())\n",
    "learn.model.append(midi3DUnet.cuda())\n",
    "learn.model.append(unstack_layers.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_m, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-med')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_L = (src.transform(get_transforms(), tfm_y=True, size=size_l)\n",
    "       .databunch(bs=bs)\n",
    "       .normalize(imagenet_stats))\n",
    "data_L.train_dl = data_L.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data_L.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_L, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)\n",
    "learn.model.append(stack_layers.cuda())\n",
    "learn.model.append(midi3DUnet.cuda())\n",
    "learn.model.append(unstack_layers.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-med');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_l, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Interpretations on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.interpret import *\n",
    "interp = SegmentationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate(data_L.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interp_show_new(self, ims:ImageSegment, classes:Collection=None, sz:int=20, cmap='tab20',\n",
    "                    title_suffix:str=None):\n",
    "        \"Show ImageSegment with color mapping labels\"\n",
    "        fig,axes=plt.subplots(1,2,figsize=(sz,sz))\n",
    "        np_im = to_np(ims.data).copy()\n",
    "        # tab20 - qualitative colormaps support max of 20 distinc colors\n",
    "        # if len(classes) > 20 close idxs map to same color\n",
    "        # image\n",
    "        if classes is not None:\n",
    "            class_idxs = [self.c2i[c] for c in classes]\n",
    "            mask = np.max(np.stack([np_im==i for i in class_idxs]),axis=0)\n",
    "            np_im = (np_im*mask).astype(np.float)\n",
    "            np_im[np.where(mask==0)] = np.nan\n",
    "        im=axes[0].imshow(np_im[0], cmap=cmap)\n",
    "\n",
    "        # labels\n",
    "        np_im_labels = list(np.unique(np_im[~np.isnan(np_im)]))\n",
    "        c = len(np_im_labels); n = math.ceil(np.sqrt(c))\n",
    "        label_im = np.array(np_im_labels + [np.nan]*(n**2-c)).reshape(n,n)\n",
    "        axes[1].imshow(label_im, cmap=cmap)\n",
    "        for i,l in enumerate([self.i2c[l] for l in np_im_labels]):\n",
    "            div,mod=divmod(i,n)\n",
    "            #l = \"\\n\".join(wrap(l,10)) if len(l) > 10 else l #bug fix\n",
    "            axes[1].text(mod, div, f\"{l}\", ha='center', color='white', fontdict={'size':sz})\n",
    "\n",
    "        if title_suffix:\n",
    "            axes[0].set_title(f\"{title_suffix}_imsegment\")\n",
    "            axes[1].set_title(f\"{title_suffix}_labels\")\n",
    "            \n",
    "import types\n",
    "def show_xyz_new(self, i, classes:list=None, sz=10):\n",
    "        'show (image, true and pred) from self.ds with color mappings, optionally only plot'\n",
    "        funcType = types.MethodType\n",
    "        self._interp_show = funcType(_interp_show_new, self)\n",
    "        x,y = self.ds[i]\n",
    "        self.ds.show_xys([x],[y], figsize=(sz/2,sz/2))\n",
    "        self._interp_show(ImageSegment(self.y_true[i]), classes, sz=sz, title_suffix='true')\n",
    "        self._interp_show(ImageSegment(self.pred_class[i][None,:]), classes, sz=sz, title_suffix='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalution of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from fastai.vision.interpret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_3D_oat.ini']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#provide config file name through cmd line\n",
    "config = configparser.ConfigParser()\n",
    "config.read('pred_3D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(config['data']['dir_to_model'])\n",
    "model = (config['data']['model'])\n",
    "#raw_img = config['data']['raw_images']\n",
    "#pred_lbl = config['data']['pred_lbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner(path, file=model)\n",
    "#learn.data.single_ds.tfmargs['size'] = None #ensure match to new image size, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsiList = (SegmentationItemList.from_folder(path)\n",
    "       .split_by_folder(train='Images', valid='Test')\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (lsiList.transform( \n",
    "        tfm_y=True, \n",
    "        size=300)\n",
    "        .databunch(bs=bs)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai3D import loader\n",
    "#data_test.train_dl = data_test.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data_test.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.valid_dl = data_test.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageDataBunch;\n",
       "\n",
       "Train: LabelList (6347 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300)\n",
       "Path: /home/suze/seed_images/Data_for_ML_Test/train/Oat_test;\n",
       "\n",
       "Valid: LabelList (2316 items)\n",
       "x: SegmentationItemList\n",
       "Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300),Image (3, 300, 300)\n",
       "y: SegmentationLabelList\n",
       "ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300),ImageSegment (1, 300, 300)\n",
       "Path: /home/suze/seed_images/Data_for_ML_Test/train/Oat_test;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In [1]: import torch In [2]: torch.cuda.current_device() Out[2]: 0 In [3]: torch.cuda.device(0) \n",
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04585619, tensor(0.9709)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate(data_test.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret\n",
    "interp = SegmentationInterpretation.from_learner(learn)#,ds_type=DatasetType.Valid)\n",
    "#interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>0.997919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>0.953543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3.0</td>\n",
       "      <td>0.561429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
