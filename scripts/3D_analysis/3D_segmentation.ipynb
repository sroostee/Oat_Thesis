{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import SequentialSampler\n",
    "from fastai3D import mysampler\n",
    "import configparser\n",
    "#from fastai3D import functions\n",
    "#from fastai3D import loader #does not comply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure and Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_3D_oat.ini']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('train_3D_oat.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/NewLabels'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/Barley_big.pkl'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/Images'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/Test'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/models'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/valid.txt'),\n",
       " PosixPath('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train/environment.yml')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(config['data']['dir'])\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path = Path('/home/suze/Documents/Thesis/seed_images/Data_for_ML_Test/train') #suze_device\n",
    "#path = Path('/home/suze/seed_images/Data_for_ML_Test//test_model/Barley_test')\n",
    "#path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_img = path/'Images' #should later be provided through ini file\n",
    "#path_lbl = path/'Labels' #should later be provided through ini file #suze_device\n",
    "#path_lbl = path/'NewLabels' #should later be provided through ini file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_img = path/config['data']['images']\n",
    "path_lbl = path/config['data']['labels']\n",
    "valid = config['data']['valid']\n",
    "classes = int(config['data']['classes'])\n",
    "model_name = config['data']['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment in case of file issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_names=get_image_files(path_img)\n",
    "#img_names[:3]\n",
    "#lbl_names=get_image_files(path_lbl)\n",
    "#lbl_names[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to find mask matching the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_mask(x):\n",
    "#    return PosixPath(str(x)\n",
    "#            .replace('train','NewLabels')\n",
    "#            .replace('valid','NewLabels'))\n",
    "            #.replace('resampled', 'labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment for error testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = open_mask(get_mask(img_names[0]))\n",
    "#mask.show()\n",
    "#img = open_image(img_names[0])\n",
    "#img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_size = np.array(mask.shape[1:])\n",
    "#src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_background(img_list):\n",
    "    include=[]\n",
    "    for img in img_list:\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total=(torch.unique(mask.data, return_counts=True))\n",
    "        if not count_total[0].tolist() == [0]:\n",
    "            include.append(img)\n",
    "    return include\n",
    "\n",
    "\"\"\"function to filter out images that only contain background; used to filter in the API; \n",
    "function has to return a boolean\n",
    "slow, every mask has to be opened to check the values\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def count_mask(img_list):\n",
    "    img_list = filter_background(img_list)\n",
    "    count_classes = defaultdict(int)\n",
    "    for img in img_list:\n",
    "        #return the occurence of every class in the mask for an image\n",
    "        mask = open_mask(get_mask(img))\n",
    "        count_total = torch.unique(mask.data, return_counts=True)\n",
    "        classes=count_total[0].tolist()\n",
    "        count_real=count_total[1].tolist()\n",
    "        for x, y in zip(classes, count_real):\n",
    "            count_classes[x] += y\n",
    "    return count_classes\n",
    "\n",
    "def img_train(path_img, valid):\n",
    "    #create a list of images not in validation set\n",
    "    path = path_img.parent\n",
    "    valid_names = loadtxt_str(path/valid)\n",
    "    img_names = get_image_files(path_img)\n",
    "    train_img = list(filter(lambda x: (x.name not in valid_names), img_names))\n",
    "    return train_img\n",
    "\n",
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0 #not interested in background\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate appropriate weights\n",
    "still needs some tweaking to exclude valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = img_train(path_img, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%time classes_count = count_mask(train_img)\n",
    "#classes_count\n",
    "#slow for big sets, make faster?\n",
    "classes_count = defaultdict(int, {0: 796196338, 1: 240902526, 2: 25500623, 3: 21869121})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all classes are represented in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(classes_count.items()) != classes: print('Not all classes present in training data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append occurences in a list \n",
    "#seems redundant\n",
    "#counts = []\n",
    "#for c in classes_count:\n",
    "#    counts.append(classes_count[c])\n",
    "#counts\n",
    "counts = [796196338, 240902526, 25500623, 21869121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_ratios =[min(counts)/x for x in counts]\n",
    "#weight_ratios\n",
    "weight_ratios = [0.02746699520740574, 0.09077995720144504, 0.8575916360945378, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load in data and apply transformations etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure settings for training\n",
    "bs = int(config['training_settings']['batch_size'])\n",
    "wd = float(config['training_settings']['wd'])\n",
    "lr = float(config['training_settings']['lr'])\n",
    "size_s = int(config['training_settings']['size_s'])\n",
    "size_m = int(config['training_settings']['size_m'])\n",
    "size_l = int(config['training_settings']['size_l'])\n",
    "epochs_s1 = int(config['training_settings']['epochs_s1'])\n",
    "epochs_s2 = int(config['training_settings']['epochs_s2'])\n",
    "epochs_m = int(config['training_settings']['epochs_m'])\n",
    "epochs_l = int(config['training_settings']['epochs_l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path)\n",
    "       #.split_subsets(train_size=0.2, valid_size=0.1)\n",
    "       .filter_by_folder(include=config['data']['images'])\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file(valid)\n",
    "       .label_from_func(get_mask, classes=list(range(classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = (src.transform(get_transforms(), tfm_y=True, size=128)\n",
    "#       .databunch(bs=1)\n",
    "#       .normalize())\n",
    "#TO DO: transforms need to be applied after batching \n",
    "data = (src.transform(tfm_y=True,\n",
    "        size=size_s)\n",
    "       .databunch(bs=1)\n",
    "       .normalize())\n",
    "#data.show_batch(4, figsize=(10,7))\n",
    "#data.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to custom sampler for order within batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai3D import loader\n",
    "data.train_dl = data.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data.train_dl), 6, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "#print(data.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set class weights\n",
    "class_weights=torch.FloatTensor(weight_ratios)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()\n",
    "#learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add 3D layers to learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False,\n",
    "               is_2d:bool = False, trans_2d:bool = False,\n",
    "               norm_type:Optional[NormType]=NormType.Batch,  use_activ:bool=True, leaky:float=None,\n",
    "               transpose:bool=False, init:Callable=nn.init.kaiming_normal_, self_attention:bool=False):\n",
    "    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and batchnorm (if `bn`) layers.\"\n",
    "    if padding is None: padding = (ks-1)//2 if not transpose else 0\n",
    "    bn = norm_type in (NormType.Batch, NormType.BatchZero)\n",
    "    if bias is None: bias = not bn\n",
    "    conv_func = nn.ConvTranspose3d if transpose else nn.Conv1d if is_1d else nn.ConvTranspose2d if trans_2d else nn.Conv2d if is_2d else nn.Conv3d\n",
    "    conv = init_default(conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding), init)\n",
    "    if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n",
    "    elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n",
    "    layers = [conv]\n",
    "    if use_activ: layers.append(relu(True, leaky=leaky))\n",
    "    if bn: layers.append((nn.BatchNorm1d if is_1d else nn.BatchNorm2d if is_2d else nn.BatchNorm3d)(nf))\n",
    "    if self_attention: layers.append(SelfAttention(nf))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "#both need to be walked through again, as batch_size dimensions don't match when predicting\n",
    "#ValueError: Expected input batch_size (512) to match target batch_size (16384).\n",
    "stack_layers = nn.Sequential(\n",
    "    Lambda(lambda x: torch.stack(tuple(x), dim = 3)),\n",
    "    Lambda(lambda x: torch.unsqueeze(x,0))\n",
    ")\n",
    "\n",
    "#unstack_layers = nn.Sequential(\n",
    "#    Lambda(lambda x: torch.transpose(x, 0 , 4)),\n",
    "#    Lambda(lambda x: x.squeeze())\n",
    "#)\n",
    "\n",
    "unstack_layers = nn.Sequential(\n",
    "    Lambda(lambda x: torch.squeeze(x, dim = 0)),\n",
    "    Lambda(lambda x: torch.stack(torch.unbind(x, dim = 3), dim =0))\n",
    "    #Lambda(lambda x: torch.stack(x, dim = 0))\n",
    ")\n",
    "\n",
    "just3D = nn.Sequential(conv3_layer(4,64),conv3_layer(64,4))\n",
    "\n",
    "mini3DUnet = SequentialEx(conv3_layer(4, 64),\n",
    "                          conv3_layer(64,128),\n",
    "                          conv3_layer(128,64),\n",
    "                         conv3_layer(64,4),\n",
    "                         MergeLayer())\n",
    "\n",
    "midi3DUnet = SequentialEx(conv3_layer(4,64),\n",
    "                          SequentialEx(conv3_layer(64,128),\n",
    "                                       (SequentialEx(conv3_layer(128,128), MergeLayer())),\n",
    "                                      conv3_layer(128,64), MergeLayer()),\n",
    "                         conv3_layer(64,4),\n",
    "                         MergeLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.append(stack_layers)\n",
    "learn.model.append(mini3DUnet)\n",
    "learn.model.append(unstack_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(cyc_len=epochs_s1, max_lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_1')\n",
    "#learn=None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_s2, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-small_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = (src.transform(tfm_y=True, size=size_m)\n",
    "       .databunch(bs=1)\n",
    "       .normalize())\n",
    "data_med.train_dl = data_med.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn=None\n",
    "gc.collect()\n",
    "learn = unet_learner(data_med, models.resnet34, metrics=metrics, wd=wd)\n",
    "learn.crit = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-small_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_m, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('stage-med')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_L = (src.transform(get_transforms(), tfm_y=True, size=size_l)\n",
    "       .databunch(bs=bs)\n",
    "       .normalize())\n",
    "data_L.train_dl = data_L.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data.train_dl), bs, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('stage-med');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(epochs_l, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.export(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Interpretations on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.interpret import *\n",
    "interp = SegmentationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses, top_idxs = interp.top_losses(sizes=(size_l,size_l))\n",
    "top_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at single worst performing picture\n",
    "i = top_idxs[0]\n",
    "df = interp._plot_intersect_cm(single_img_cm[i], f\"Ratio of Intersection given True Label, Image:{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _interp_show_new(self, ims:ImageSegment, classes:Collection=None, sz:int=20, cmap='tab20',\n",
    "                    title_suffix:str=None):\n",
    "        \"Show ImageSegment with color mapping labels\"\n",
    "        fig,axes=plt.subplots(1,2,figsize=(sz,sz))\n",
    "        np_im = to_np(ims.data).copy()\n",
    "        # tab20 - qualitative colormaps support max of 20 distinc colors\n",
    "        # if len(classes) > 20 close idxs map to same color\n",
    "        # image\n",
    "        if classes is not None:\n",
    "            class_idxs = [self.c2i[c] for c in classes]\n",
    "            mask = np.max(np.stack([np_im==i for i in class_idxs]),axis=0)\n",
    "            np_im = (np_im*mask).astype(np.float)\n",
    "            np_im[np.where(mask==0)] = np.nan\n",
    "        im=axes[0].imshow(np_im[0], cmap=cmap)\n",
    "\n",
    "        # labels\n",
    "        np_im_labels = list(np.unique(np_im[~np.isnan(np_im)]))\n",
    "        c = len(np_im_labels); n = math.ceil(np.sqrt(c))\n",
    "        label_im = np.array(np_im_labels + [np.nan]*(n**2-c)).reshape(n,n)\n",
    "        axes[1].imshow(label_im, cmap=cmap)\n",
    "        for i,l in enumerate([self.i2c[l] for l in np_im_labels]):\n",
    "            div,mod=divmod(i,n)\n",
    "            #l = \"\\n\".join(wrap(l,10)) if len(l) > 10 else l #bug fix\n",
    "            axes[1].text(mod, div, f\"{l}\", ha='center', color='white', fontdict={'size':sz})\n",
    "\n",
    "        if title_suffix:\n",
    "            axes[0].set_title(f\"{title_suffix}_imsegment\")\n",
    "            axes[1].set_title(f\"{title_suffix}_labels\")\n",
    "            \n",
    "import types\n",
    "def show_xyz_new(self, i, classes:list=None, sz=10):\n",
    "        'show (image, true and pred) from self.ds with color mappings, optionally only plot'\n",
    "        funcType = types.MethodType\n",
    "        self._interp_show = funcType(_interp_show_new, self)\n",
    "        x,y = self.ds[i]\n",
    "        self.ds.show_xys([x],[y], figsize=(sz/2,sz/2))\n",
    "        self._interp_show(ImageSegment(self.y_true[i]), classes, sz=sz, title_suffix='true')\n",
    "        self._interp_show(ImageSegment(self.pred_class[i][None,:]), classes, sz=sz, title_suffix='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcType = types.MethodType\n",
    "interp.show_xyz = funcType(show_xyz_new, interp)\n",
    "interp.show_xyz(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for xb, yb in data.train_dl:\n",
    "#    #out = model[xb]\n",
    "#    out = (learn.model(*[xb]))\n",
    "#    #outy = (model(*[yb]))\n",
    "#    #print(xb.size())\n",
    "#    print(out.size())\n",
    "#    #print(yb.size())\n",
    "#    #print(outy.size())\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_batch = data.one_batch(detach=True)\n",
    "#a_batch\n",
    "#a_batch[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.load('stage-1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrs = slice(3e-5/100, 3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(4, lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fastai.vision.interpret import *\n",
    "#interp = SegmentationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_losses, top_idxs = interp.top_losses(sizes=(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_cm, single_img_cm = interp._generate_confusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = interp._plot_intersect_cm(mean_cm, \"Mean of Ratio of Intersection given True Label\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
