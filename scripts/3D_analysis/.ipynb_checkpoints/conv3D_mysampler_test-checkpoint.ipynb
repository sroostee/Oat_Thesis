{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "#from torch.utils import data\n",
    "from fastai.vision import *\n",
    "import warnings\n",
    "from torch.utils.data import SequentialSampler\n",
    "from fastai3D import mysampler\n",
    "#from fastai3D import loader #does not comply \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = untar_data(URLs.CAMVID_TINY)#small segmentation test set\n",
    "#path_img = path/'images' #should later be provided through ini file\n",
    "#path_lbl = path/'labels' #should later be provided through ini file\n",
    "#path.ls()\n",
    "#parts = np.loadtxt(path/'codes.txt', dtype=str); parts #should later be provided through ini file\n",
    "#these setting should later be set through an ini file\n",
    "#bs = 4 #batch size, set smaller when running out of memory\n",
    "#get_y_fn = lambda x: path_lbl/f'{x.stem}_P{x.suffix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Block API\n",
    "#data = SegmentationItemList.from_folder(path_img).split_by_rand_pct(valid_pct=0.2, seed=42) #for seed validation use an unseen kernel? because slices are not independent of eachother (like video)\n",
    "#data = data.label_from_func(get_y_fn, classes=parts)\n",
    "#data = data.transform(get_transforms(), tfm_y=True, size=128) #for the seeds set get_transforms(flip_vert:bool=True) and perhaps something with max_rotate()\n",
    "#data = data.databunch(bs=bs, path=path)\n",
    "#data = data.normalize() #think about this for the seeds\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a quick look at the data (optional)\n",
    "#data.show_batch(rows=2, figsize=(7,5))\n",
    "#learn = unet_learner(data, models.resnet18) #think of other arguments to pass, such as custom defined metrics\n",
    "#learn.fit_one_cycle(cyc_len=10, max_lr=1e-2) #what other argumenets should be passed in? how many epochs? what lr\n",
    "#learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Seed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/suze/seed_images/Data_for_ML_Test/test_model/Barley_test')\n",
    "path_img = path/'Images' #should later be provided through ini file\n",
    "path_lbl = path/'NewLabels' #should later be provided through ini file\n",
    "classes = range(4)\n",
    "\n",
    "def get_mask(img):\n",
    "    return (path_lbl)/img.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_names=get_image_files(path_img)\n",
    "#img_names[:3]\n",
    "#lbl_names=get_image_files(path_lbl)\n",
    "#lbl_names[:3]\n",
    "\n",
    "#img = open_image(img_names[4])\n",
    "#img.show()\n",
    "\n",
    "#mask = open_mask(get_mask(img_names[0]))\n",
    "#mask.show()\n",
    "\n",
    "#src_size = np.array(mask.shape[1:])\n",
    "#src_size,mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"function to filter out images that only contain background; used to filter in the data block API; \n",
    "function has to return a boolean\n",
    "slow, every mask has to be opened to check the values\"\"\"\n",
    "def check_back(img):\n",
    "    mask = open_mask(get_mask(img))\n",
    "    count_total=(torch.unique(mask.data, return_counts=True))\n",
    "    if not count_total[0].tolist() == [0]:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path)\n",
    "       #.split_subsets(train_size=0.2, valid_size=0.1)\n",
    "       .filter_by_func(check_back)\n",
    "       .split_by_fname_file('valid.txt')\n",
    "       .label_from_func(get_mask, classes=list(range(4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (src.transform(get_transforms(), tfm_y=True, size=128)\n",
    "       .databunch(bs=1)\n",
    "       .normalize())\n",
    "#data.show_batch(4, figsize=(10,7))\n",
    "data.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (src.transform(get_transforms(flip_vert=True), tfm_y=True, size=128)\n",
    "       .databunch(bs=3)\n",
    "       .normalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f3b92bf87067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai3D\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmysampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedBatchSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/learnfastai/lib/python3.7/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"Create a new copy of `self` with `kwargs` replacing current values.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnew_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         return DeviceDataLoader(self.dl.__class__(self.dl.dataset, **new_kwargs), self.device, self.tfms,\n\u001b[0m\u001b[1;32m     65\u001b[0m                                 self.collate_fn)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Oat_Thesis/scripts/3D_analysis/fastai3D/loader.py\u001b[0m in \u001b[0;36mintercept_args\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m                                                 \u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'collate_fn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pin_memory'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \t\t\t\t\t\t'drop_last': drop_last, 'timeout':timeout, 'worker_init_fn':worker_init_fn}\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mold_dl_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintercept_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Oat_Thesis/scripts/3D_analysis/fastai3D/loader.py\u001b[0m in \u001b[0;36mmy__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0;31m# auto_collation with custom batch_sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \t\t\t\traise ValueError('batch_sampler option is mutually exclusive '\n\u001b[0m\u001b[1;32m     87\u001b[0m                                                                  \u001b[0;34m'with batch_size, shuffle, sampler, and '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \t\t\t\t\t\t\t\t 'drop_last')\n",
      "\u001b[0;31mValueError\u001b[0m: batch_sampler option is mutually exclusive with batch_size, shuffle, sampler, and drop_last"
     ]
    }
   ],
   "source": [
    "from fastai3D import loader\n",
    "data.train_dl = data.train_dl.new(shuffle=False, drop_last=False, sampler=None, batch_sampler=mysampler.OrderedBatchSampler(SequentialSampler(data.train_dl), 1, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed data to Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_seeds(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != 0\n",
    "    return (input.argmax(dim=1)[mask] == target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=acc_seeds\n",
    "wd=1e-2\n",
    "learn = unet_learner(data, models.resnet18, metrics=metrics, wd=wd)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(cyc_len=1, max_lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_output = learn.data\n",
    "learn_batch = learn_output.one_batch()\n",
    "learn_batch[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch = data.one_batch(detach=True)\n",
    "a_batch\n",
    "a_batch[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torch.Tensor(a_batch[0][0])\n",
    "img2 = torch.Tensor(a_batch[0][1])\n",
    "img1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_img = img1\n",
    "two_img = torch.stack((img1, img2), dim = 3)\n",
    "two_img.unsqueeze_(0)\n",
    "two_img.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_3conv = nn.Conv3d(in_channels=3, out_channels=64, kernel_size=(1,1,1), stride=1, padding=0)\n",
    "#input = torch.randn(1,3 , 6, 10, 10)\n",
    "#print(input)\n",
    "#output = simple_3conv(input)\n",
    "#output.size()\n",
    "#torch.Size([1, 30, 3, 10, 10])\n",
    "simple_2conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3,3), stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = simple_2conv(a_batch[0])\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = simple_3conv(two_img)\n",
    "output3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.layers import *\n",
    "my_conv_layer1 = conv_layer(3, 64)\n",
    "my_conv_layer2 = conv_layer(64,128)\n",
    "con_test = my_conv_layer2(my_conv_layer1(a_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_relu = relu(leaky=None)\n",
    "this_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3_layer(ni:int, nf:int, ks:int=3, stride:int=1, padding:int=None, bias:bool=None, is_1d:bool=False,\n",
    "               is_2d:bool = False, trans_2d:bool = False,\n",
    "               norm_type:Optional[NormType]=NormType.Batch,  use_activ:bool=True, leaky:float=None,\n",
    "               transpose:bool=False, init:Callable=nn.init.kaiming_normal_, self_attention:bool=False):\n",
    "    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and batchnorm (if `bn`) layers.\"\n",
    "    if padding is None: padding = (ks-1)//2 if not transpose else 0\n",
    "    bn = norm_type in (NormType.Batch, NormType.BatchZero)\n",
    "    if bias is None: bias = not bn\n",
    "    conv_func = nn.ConvTranspose3d if transpose else nn.Conv1d if is_1d else nn.ConvTranspose2d if trans_2d else nn.Conv2d if is_2d else nn.Conv3d\n",
    "    conv = init_default(conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding), init)\n",
    "    if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n",
    "    elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n",
    "    layers = [conv]\n",
    "    if use_activ: layers.append(relu(True, leaky=leaky))\n",
    "    if bn: layers.append((nn.BatchNorm1d if is_1d else nn.BatchNorm2d if is_2d else nn.BatchNorm3d)(nf))\n",
    "    if self_attention: layers.append(SelfAttention(nf))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv_layer1 = conv_layer(3, 64)\n",
    "my_conv_layer2 = conv_layer(64,128)\n",
    "con_test = my_conv_layer2(my_conv_layer1(a_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_conv_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_3conv_l1 = conv3_layer(3,64)\n",
    "my_3conv_l2 = conv3_layer(64,128)\n",
    "con3_test= my_3conv_l2(my_3conv_l1(two_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con3_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.models.unet import *\n",
    "from fastai.callbacks.hooks import *\n",
    "class Unet3Block(Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, hook:Hook, final_div:bool=True, blur:bool=False, leaky:float=None,\n",
    "                 self_attention:bool=False, **kwargs):\n",
    "        self.hook = hook\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, leaky=leaky, **kwargs)\n",
    "        self.bn = batchnorm_2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = conv3_layer(ni, nf, leaky=leaky, **kwargs)\n",
    "        self.conv2 = conv3_layer(nf, nf, leaky=leaky, self_attention=self_attention, **kwargs)\n",
    "        self.relu = relu(leaky=leaky)\n",
    "        \n",
    "    def forward(self, up_in:Tensor) -> Tensor:\n",
    "        s = self.hook.stored\n",
    "        up_out = self.shuf(up_in)\n",
    "        ssh = s.shape[-2:]\n",
    "        if ssh != up_out.shape[-2:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-2:], mode='nearest')\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test Unet3Block\n",
    "unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa, **kwargs).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dynamic3DUnet(SequentialEx):\n",
    "    \"Create a volumetric U-Net from a given architecture. Completely based of the Dynamic Unet from fastai.\"\n",
    "    def __init__(self, encoder:nn.Module, n_classes:int, img_size:Tuple[int,int]=(256,256), blur:bool=False, blur_final=True, self_attention:bool=False,\n",
    "                 y_range:Optional[Tuple[float,float]]=None,\n",
    "                 last_cross:bool=True, bottle:bool=False, **kwargs):\n",
    "        imsize = img_size\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs], detach=False)\n",
    "        x = dummy_eval(encoder, imsize).detach()\n",
    "\n",
    "        ni = sfs_szs[-1][1]\n",
    "        middle_conv = nn.Sequential(conv3_layer(ni, ni*2, **kwargs),\n",
    "                                    conv3_layer(ni*2, ni, **kwargs)).eval()\n",
    "        x = middle_conv(x)\n",
    "        layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]\n",
    "\n",
    "        for i,idx in enumerate(sfs_idxs):\n",
    "            not_final = i!=len(sfs_idxs)-1\n",
    "            up_in_c, x_in_c = int(x.shape[1]), int(sfs_szs[idx][1])\n",
    "            do_blur = blur and (not_final or blur_final)\n",
    "            sa = self_attention and (i==len(sfs_idxs)-3)\n",
    "            unet_block = UnetBlock(up_in_c, x_in_c, self.sfs[i], final_div=not_final, blur=do_blur, self_attention=sa,\n",
    "                                   **kwargs).eval()\n",
    "            layers.append(unet_block)\n",
    "            x = unet_block(x)\n",
    "\n",
    "        ni = x.shape[1]\n",
    "        if imsize != sfs_szs[0][-2:]: layers.append(PixelShuffle_ICNR(ni, **kwargs))\n",
    "        x = PixelShuffle_ICNR(ni)(x)\n",
    "        if imsize != x.shape[-2:]: layers.append(Lambda(lambda x: F.interpolate(x, imsize, mode='nearest')))\n",
    "        if last_cross:\n",
    "            layers.append(MergeLayer(dense=True))\n",
    "            ni += in_channels(encoder)\n",
    "            layers.append(res_block(ni, bottle=bottle, **kwargs))\n",
    "        layers += [conv_layer(ni, n_classes, ks=1, use_activ=False, **kwargs)]\n",
    "        if y_range is not None: layers.append(SigmoidRange(*y_range))\n",
    "        super().__init__(*layers)\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
