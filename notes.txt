data: Barley BB5

no seed value

data = (src.transform(get_transforms(), tfm_y=True, size=256)
       .databunch(bs=8)
       .normalize())

model: resnet34
wd = 1e-2
lr = 1e-4

after 4 epochs: 
train_loss: 0.053970
valid_loss: 0.027840
acc_seeds: 0.981817 	
time: 13:12+13:59+14:10+14:22

unfreeze final layer and continue for 2 epochs with
lr = slice(1e-4/100, 1e-4)

train_loss: 0.054219
valid_loss: 0.025018
acc_seeds: 0.983520
time: 13:09+

###################################

first training run with size=128 speeds up 4x
then second run with size=256

after 4 epochs: lr = 1e-4
train_loss: 0.083491
valid_loss: 0.050482
acc_seeds: 0.967652 	
time: 3:54+3:50+3:50+3:55

after 2 epochs: lr = slice(1e-4/100, 1e-4)
train_loss: 0.045032
valid_loss: 0.025489
acc_seeds: 0.982632
time: 13:24+13:37

###########################################

first run size=128
after 4 epochs: lr = 1e-4
train_loss: 0.087520
valid_loss: 0.052396
acc_seeds: 0.964695	
time: 3:51+3:45+3:50+3:53

seconds run size=128
after 2 epochs: lr = slice(1e-4/100, 1e-4)
train_loss: 0.076072
valid_loss: 0.039965
acc_seeds: 0.971990
time: 4:14+4:18

third run size=256
after 2 epochs: lr = slice(1e-4/100, 1e-4)
train_loss: 0.041505
valid_loss: 0.021361 	
acc_seeds: 0.982427
time: 13:22+13:25

########################################

repeat above procedure with custom weights
loss = cross entropy
weights = [0.1, 0.7, 1, 1] #guessed values

final results:

1 	0.048660 	0.022763 	0.982864 	13:19

########################################

repeat with calculated weights
[0.025899280575539568, 0.20702517873795462, 1.0, 0.19762611275964392]

final results:
1 	0.059577 	0.022862 	0.981667 	00:32

#############################################

after excluding data with only background
weights:

[0.031095499613658085, 0.06401058961337106, 0.606182866200015, 1.0]

1 	0.043681 	0.023416 	0.984054 	00:28

################################################

following above but adding 2 more epochs with 
data size=300

1 	0.035676 	0.020052 	0.986091 	00:39



##############################

